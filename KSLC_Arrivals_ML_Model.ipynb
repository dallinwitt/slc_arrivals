{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Arrival Status of Flights at SLC Airport: Supervised ML\n",
    "\n",
    "# Introduction\n",
    "Late flight arrivals are a hassle, not only for travelers, but also for the people waiting for them at their destination. Fortunately, the [US Bureau of Transportation Statistics](https://www.bts.gov/) has information about flight arrival and departure statistics going back to the 80s. I combined this with distance information obtained from [OpenFlights](https://openflights.org/), and weather information from [NOAA](https://www.ncdc.noaa.gov/cdo-web/search) to create a set of inputs that would help predict whether or not a flight would arrive on time.\n",
    "\n",
    "# 1 Loading Data\n",
    "\n",
    "## 1.1 Importing Packages\n",
    "For this project we will use four packages during import (Pandas, Numpy, pyplot, and glob), and ten packages from Scikit-Learn during construction and deployment of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages: pandas, numpy, pyplot, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Importing Data\n",
    "### 1.2.1 Arrival Data\n",
    "Begin by generating a list of the files from the USBTS. We will createa an empty list of dataframes, then populate that list with each dataframe as we import it. We can then concatenate the seven dataframes together, and replace teh column names with more usable names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Detailed_Statistics_Arrivals_AA.csv',\n",
       " 'Detailed_Statistics_Arrivals_AS.csv',\n",
       " 'Detailed_Statistics_Arrivals_B6.csv',\n",
       " 'Detailed_Statistics_Arrivals_DL.csv',\n",
       " 'Detailed_Statistics_Arrivals_F9.csv',\n",
       " 'Detailed_Statistics_Arrivals_UA.csv',\n",
       " 'Detailed_Statistics_Arrivals_WN.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use glob to generate list of arrival detail files\n",
    "arr_stat_list = glob.glob(\"Detailed*.csv\")\n",
    "arr_stat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in CSVs into dummy list\n",
    "cols = [0, 1, 4, 5, 7, 8, 9]\n",
    "df_list = []\n",
    "\n",
    "for i in range(7):\n",
    "    file = arr_stat_list[i]\n",
    "    df_list.append(pd.read_csv(file, \n",
    "                engine = 'python',\n",
    "                usecols = cols, \n",
    "                dtype = {'Origin Airport':'category'},\n",
    "                skiprows = 7, \n",
    "                parse_dates = [[1, 3]],\n",
    "                skipfooter = 1\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate list into single df, reset index, drop original index, rename cols, \n",
    "#and make the carrier and origin columns categoricals\n",
    "arr_data = pd.concat(df_list)\n",
    "arr_data.reset_index(inplace = True)\n",
    "arr_data = arr_data.drop(\"index\", axis = 1)\n",
    "arr_data.columns = ['scheduled_arr', 'carrier', 'origin', 'scheduled_elapsed', 'actual_elapsed', 'arr_delay']\n",
    "arr_data[['carrier', 'origin']] = arr_data[['carrier', 'origin']].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also read in the distances between KSLC and every other airport that is serviced from there. We can then merge this dataframe with the arrivals dataframe, so each flight hsa a distance associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scheduled_arr</th>\n",
       "      <th>carrier</th>\n",
       "      <th>origin</th>\n",
       "      <th>scheduled_elapsed</th>\n",
       "      <th>actual_elapsed</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988-01-01 11:02:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>ORD</td>\n",
       "      <td>197</td>\n",
       "      <td>211</td>\n",
       "      <td>22</td>\n",
       "      <td>1245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1988-01-01 11:17:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>DFW</td>\n",
       "      <td>160</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-01-01 12:31:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>DFW</td>\n",
       "      <td>154</td>\n",
       "      <td>174</td>\n",
       "      <td>21</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988-01-01 15:00:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>JAC</td>\n",
       "      <td>52</td>\n",
       "      <td>50</td>\n",
       "      <td>-3</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-01-01 07:50:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>IDA</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043276</th>\n",
       "      <td>2019-12-31 11:30:00</td>\n",
       "      <td>WN</td>\n",
       "      <td>MDW</td>\n",
       "      <td>210</td>\n",
       "      <td>191</td>\n",
       "      <td>-9</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043277</th>\n",
       "      <td>2019-12-31 15:45:00</td>\n",
       "      <td>WN</td>\n",
       "      <td>PHX</td>\n",
       "      <td>100</td>\n",
       "      <td>86</td>\n",
       "      <td>-16</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043278</th>\n",
       "      <td>2019-12-31 10:00:00</td>\n",
       "      <td>WN</td>\n",
       "      <td>LAX</td>\n",
       "      <td>110</td>\n",
       "      <td>118</td>\n",
       "      <td>-1</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043279</th>\n",
       "      <td>2019-12-31 22:20:00</td>\n",
       "      <td>WN</td>\n",
       "      <td>SAN</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043280</th>\n",
       "      <td>2019-12-31 21:20:00</td>\n",
       "      <td>WN</td>\n",
       "      <td>SJC</td>\n",
       "      <td>100</td>\n",
       "      <td>106</td>\n",
       "      <td>16</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2043281 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              scheduled_arr carrier origin  scheduled_elapsed  actual_elapsed  \\\n",
       "0       1988-01-01 11:02:00      AA    ORD                197             211   \n",
       "1       1988-01-01 11:17:00      AA    DFW                160             170   \n",
       "2       1988-01-01 12:31:00      AA    DFW                154             174   \n",
       "3       1988-01-01 15:00:00      AA    JAC                 52              50   \n",
       "4       1988-01-01 07:50:00      AA    IDA                 48               0   \n",
       "...                     ...     ...    ...                ...             ...   \n",
       "2043276 2019-12-31 11:30:00      WN    MDW                210             191   \n",
       "2043277 2019-12-31 15:45:00      WN    PHX                100              86   \n",
       "2043278 2019-12-31 10:00:00      WN    LAX                110             118   \n",
       "2043279 2019-12-31 22:20:00      WN    SAN                110             110   \n",
       "2043280 2019-12-31 21:20:00      WN    SJC                100             106   \n",
       "\n",
       "         arr_delay  distance  \n",
       "0               22      1245  \n",
       "1                9       987  \n",
       "2               21       987  \n",
       "3               -3       204  \n",
       "4                0       188  \n",
       "...            ...       ...  \n",
       "2043276         -9      1254  \n",
       "2043277        -16       507  \n",
       "2043278         -1       589  \n",
       "2043279          6       626  \n",
       "2043280         16       583  \n",
       "\n",
       "[2043281 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in airport distance info\n",
    "distance = pd.read_csv('SLC_routes.csv')\n",
    "arr_data_dist = arr_data.merge(distance, how = 'left', left_on = 'origin', right_on = 'faa_code')\n",
    "arr_data_dist.drop('faa_code', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will desconstruct the datetime associated with each flight, so we can have separate columns for hour, date, day, day of year, and year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add cols: 'arr_DateHour', 'date', 'day_name', and 'day_of_year' and 'sceduled_hour'\n",
    "arr_data_dist['arr_DateHour'] = arr_data_dist['scheduled_arr'].dt.round('H')\n",
    "arr_data_dist['date'] = arr_data_dist['scheduled_arr'].dt.date\n",
    "arr_data_dist['day_name'] = arr_data_dist['scheduled_arr'].dt.day_name()\n",
    "arr_data_dist['day_of_year'] = arr_data_dist['scheduled_arr'].dt.dayofyear\n",
    "arr_data_dist['date'] = pd.to_datetime(arr_data_dist['date'])\n",
    "arr_data_dist['scheduled_hour'] = arr_data_dist['scheduled_arr'].dt.hour\n",
    "arr_data_dist['year'] = arr_data_dist['scheduled_arr'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Precipitation Data\n",
    "The precipitation data is pretty spotty, foremost because most days have no precipitation. First, we will resample the data so that every hour is represented. Then we will fill all NAs with 0s. This dataframe is now ready to be merged into the next weather dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in precip data\n",
    "precip = pd.read_csv('kslc_precip_data.csv',\n",
    "                    usecols = [2, 3])\n",
    "precip[\"DATE\"] = pd.to_datetime(precip[\"DATE\"])\n",
    "\n",
    "#create a dt index, resample the data to hourly, and replace NaN with 0\n",
    "precip.set_index('DATE', inplace = True)\n",
    "precip_hour = precip.resample('H').asfreq()\n",
    "precip_hour.fillna(0, inplace = True)\n",
    "precip_hour.reset_index(inplace = True)\n",
    "precip_hour.columns = ['DateHour', 'HourPrecip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Weather Data\n",
    "The weather data is hourly, so it is already aligned with the resampled precipitation frame. For all missing average wind values, we will just use the overall mean avg_wind value. We can assume that a null values for snow or water on the ground is 0. Lastly, we will fill null T_avg values with the mean of T_max and T_min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in weather data\n",
    "weather = pd.read_csv('kslc_daily_weather_data.csv')\n",
    "weather[\"date\"] = pd.to_datetime(weather[\"date\"])\n",
    "weather.drop('wind_fastest_1min', axis = 1, inplace = True)\n",
    "\n",
    "#fillna on the various missing values with reasonable substitutes\n",
    "weather[\"avg_wind\"].fillna(weather[\"avg_wind\"].mean(), inplace = True)\n",
    "weather['water_equiv_on_grd'].fillna(0, inplace = True)\n",
    "weather['snowfall'].fillna(0, inplace = True)\n",
    "weather[\"tavg\"].fillna(((weather.tmax + weather.tmin) / 2), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Merge Data\n",
    "Now that all the weather data is hourly and complete, we can merge each of them into the arrivals frame, based on the hour and day that the flight was scheduled to arrive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge arr_data_dist and precip_hour on 'arr_DateHour' and 'DateHour'\n",
    "arr_precip_merge = arr_data_dist.merge(precip_hour, how = 'left', left_on = 'arr_DateHour', right_on = 'DateHour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge arr_precip_merge and weather on 'date'\n",
    "SLC_arrival_merge = arr_precip_merge.merge(weather, how = 'left', on = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['scheduled_arr', 'carrier', 'origin', 'scheduled_elapsed',\n",
       "       'actual_elapsed', 'arr_delay', 'faa_code', 'distance', 'arr_DateHour',\n",
       "       'date', 'day_name', 'day_of_year', 'scheduled_hour', 'year', 'DateHour',\n",
       "       'HourPrecip', 'avg_wind', 'precip', 'snowfall', 'tavg', 'tmax', 'tmin',\n",
       "       'water_equiv_on_grd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLC_arrival_merge.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a separate dataframe that only contains data that will be used as inputs, including both numericals and categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new df with only the info that will be used in the ML model, but keep categoricals for now\n",
    "SLC_arr_ml_cat = SLC_arrival_merge[['carrier', \n",
    "                                    'scheduled_elapsed', \n",
    "                                    'distance', \n",
    "                                    'year',\n",
    "                                    'day_name', \n",
    "                                    'day_of_year', \n",
    "                                    'scheduled_hour', \n",
    "                                    'HourPrecip', \n",
    "                                    'avg_wind',\n",
    "                                    'precip', \n",
    "                                    'snowfall',\n",
    "                                    'tavg',\n",
    "                                    'tmax',\n",
    "                                    'tmin', \n",
    "                                    'water_equiv_on_grd',\n",
    "                                    'arr_delay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillna the HourPrecip col with the mean for the column and create a column for \"ontime\"\n",
    "SLC_arr_ml_cat['HourPrecip'].fillna(SLC_arr_ml_cat['HourPrecip'].mean(), inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2043281 entries, 0 to 2043280\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Dtype   \n",
      "---  ------              -----   \n",
      " 0   carrier             category\n",
      " 1   scheduled_elapsed   int64   \n",
      " 2   distance            int64   \n",
      " 3   year                int64   \n",
      " 4   day_name            object  \n",
      " 5   day_of_year         int64   \n",
      " 6   scheduled_hour      int64   \n",
      " 7   HourPrecip          float64 \n",
      " 8   avg_wind            float64 \n",
      " 9   precip              float64 \n",
      " 10  snowfall            float64 \n",
      " 11  tavg                float64 \n",
      " 12  tmax                int64   \n",
      " 13  tmin                int64   \n",
      " 14  water_equiv_on_grd  float64 \n",
      " 15  arr_delay           int64   \n",
      "dtypes: category(1), float64(6), int64(8), object(1)\n",
      "memory usage: 251.4+ MB\n"
     ]
    }
   ],
   "source": [
    "SLC_arr_ml_cat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLC_arr_ml_cat['ontime'] = (SLC_arr_ml_cat['arr_delay'] <= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carrier</th>\n",
       "      <th>scheduled_elapsed</th>\n",
       "      <th>distance</th>\n",
       "      <th>year</th>\n",
       "      <th>day_name</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>scheduled_hour</th>\n",
       "      <th>HourPrecip</th>\n",
       "      <th>avg_wind</th>\n",
       "      <th>precip</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>water_equiv_on_grd</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>ontime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA</td>\n",
       "      <td>197</td>\n",
       "      <td>1245</td>\n",
       "      <td>1988</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>160</td>\n",
       "      <td>987</td>\n",
       "      <td>1988</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA</td>\n",
       "      <td>154</td>\n",
       "      <td>987</td>\n",
       "      <td>1988</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA</td>\n",
       "      <td>52</td>\n",
       "      <td>204</td>\n",
       "      <td>1988</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA</td>\n",
       "      <td>48</td>\n",
       "      <td>188</td>\n",
       "      <td>1988</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  carrier  scheduled_elapsed  distance  year day_name  day_of_year  \\\n",
       "0      AA                197      1245  1988   Friday            1   \n",
       "1      AA                160       987  1988   Friday            1   \n",
       "2      AA                154       987  1988   Friday            1   \n",
       "3      AA                 52       204  1988   Friday            1   \n",
       "4      AA                 48       188  1988   Friday            1   \n",
       "\n",
       "   scheduled_hour  HourPrecip  avg_wind  precip  snowfall  tavg  tmax  tmin  \\\n",
       "0              11         0.0      9.17     0.0       0.0  17.0    27     7   \n",
       "1              11         0.0      9.17     0.0       0.0  17.0    27     7   \n",
       "2              12         0.0      9.17     0.0       0.0  17.0    27     7   \n",
       "3              15         0.0      9.17     0.0       0.0  17.0    27     7   \n",
       "4               7         0.0      9.17     0.0       0.0  17.0    27     7   \n",
       "\n",
       "   water_equiv_on_grd  arr_delay  ontime  \n",
       "0                 0.5         22   False  \n",
       "1                 0.5          9   False  \n",
       "2                 0.5         21   False  \n",
       "3                 0.5         -3    True  \n",
       "4                 0.5          0    True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLC_arr_ml_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export this data fram to CSV as both a checkpoint, and as a way to easily call it in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLC_arr_ml_cat.to_csv(\"KSLC_arrivals_tidy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Preparation\n",
    "Reimport the data for further cleaning. Drop the old index, and separate out the numerical and categorical columns. Scale all the numerical columns so they have a mean of 0 and standard deviation of 1. Then remerge the numerical and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in KSCL arrivals data\n",
    "df = pd.read_csv(\"KSLC_arrivals_tidy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the old index\n",
    "df.drop(\"Unnamed: 0\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create separate dfs with the numeric and categorical variables and scale the numeric df\n",
    "num_cols = ['scheduled_elapsed', 'distance', 'year','day_of_year', \n",
    "                 'scheduled_hour', 'HourPrecip',\n",
    "                 'snowfall', 'water_equiv_on_grd']\n",
    "cat_cols = ['carrier', 'day_name', 'ontime']\n",
    "\n",
    "df_numeric = df[num_cols]\n",
    "df_cat = df[cat_cols]\n",
    "\n",
    "npa_num_scaled = scale(df_numeric)\n",
    "df_num_scaled = pd.DataFrame(npa_num_scaled)\n",
    "df_num_scaled.columns = num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the numeric and categorical dfs\n",
    "df_scaled = pd.concat([df_num_scaled, df_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dummy variables (one-hot encoded vectors) from the categoricals (airline, day of week, and arrival status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy variables for \"carrier\" and \"day_name\"\n",
    "df_dummies = pd.get_dummies(df_scaled, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 400k flights from df_dummies\n",
    "df_dummies_sample = df_dummies.sample(500000, random_state = 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out the indepenedent variables form the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate out the features and target\n",
    "X = df_dummies_sample.drop('ontime', axis = 1)\n",
    "y = df_dummies_sample['ontime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform principle component analysis (PCA) to determine the 8 most independent variables. For example, distance and expected flight time will largely provide the same information to the ML model, so we should only need one of those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perfrom PCA on the dataset\n",
    "pca = PCA(n_components = 8)\n",
    "pca.fit(df_dummies_sample)\n",
    "X_transformed = pca.transform(df_dummies_sample)\n",
    "print(X_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a logistic regression model, and fit it using a train-test split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a LogisticRegression classifier called logreg\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a fit based on the training data\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test data through the logistic model, and compare the predicted outputs with the actual outputs using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data from the confusion matrix, assiging the color of the cell to the percent accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(logreg, X_test, y_test, display_labels = ['late', 'on time'], normalize = 'true', cmap = 'Blues')\n",
    "plt.savefig('cm.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification report, which shows the precision and recall for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Reciever Operating Characteristic (ROC) curve, and include the area under the curve as an annotation on the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copute predicted probabilities\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "#get roc-auc\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "#create annotation for plot\n",
    "annot = 'Area Under Curve: {:.3f}'.format(roc_auc)\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.annotate(annot, (0.5, 0.25), c = \"DarkRed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
